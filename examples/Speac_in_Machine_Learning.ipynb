{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speac in Machine Learning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N7qbioiHzc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234890ae-794c-47bd-de6a-446ce5bd0753"
      },
      "source": [
        "!pip uninstall speac"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling speac-1.0.13:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/speac-1.0.13.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/speac/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled speac-1.0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt6Q2WigH9Mc",
        "outputId": "f4d0a6ae-3346-444f-b68a-837fb73f1df8"
      },
      "source": [
        "!pip install speac"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting speac\n",
            "  Downloading https://files.pythonhosted.org/packages/71/99/254d24f3acb2b5f9c361233d788eb09caa6d08c0454b61fabb2888f53957/speac-1.0.14-py3-none-any.whl\n",
            "Installing collected packages: speac\n",
            "Successfully installed speac-1.0.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwCFD_NjH_f-"
      },
      "source": [
        "import speac\n",
        "import music21\n",
        "\n",
        "from speac.chopin_33_3 import CHOPIN_33_3\n",
        "from speac.chopin_63_2 import CHOPIN_63_2\n",
        "from speac.chopin_67_4 import CHOPIN_67_4\n",
        "\n",
        "from speac.top_level import *\n",
        "from music21 import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWX84TjEIBXL"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import glob\n",
        "import numpy\n",
        "from tensorflow import keras\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "import timeit"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJxv2xw_IFMj",
        "outputId": "3d9df463-617f-4ef6-b825-6f858a3ac7c0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcdDBYc_ei_T"
      },
      "source": [
        "Чтение хоралов Баха и их запись в форму, удобную для работы со SPEAC-анализом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW6rM4lzIJpj",
        "outputId": "20858b69-8835-414b-85e7-fb3e98cdf197"
      },
      "source": [
        "  chorales = []\n",
        "  signatures = []\n",
        "  k = 1\n",
        "\n",
        "  majors = dict([(\"A-\", 4),(\"G#\", 4),(\"A\", 3),(\"A#\", 2),(\"B-\", 2),(\"B\", 1),\n",
        "                 (\"C\", 0),(\"C#\", -1),(\"D-\", -1),(\"D\", -2),(\"D#\", -3),(\"E-\", -3),\n",
        "                 (\"E\", -4),(\"F\", -5),(\"F#\", 6),(\"G-\", 6),(\"G\", 5)])   \n",
        "\n",
        "  minors = dict([(\"G#\", 1), (\"A-\", 1),(\"A\", 0),(\"A#\", -1),(\"B-\", -1),(\"B\", -2),\n",
        "                 (\"C\", -3),(\"C#\", -4),(\"D-\", -4),(\"D\", -5),(\"D#\", 6),(\"E-\", 6),\n",
        "                 (\"E\", 5),(\"F\", 4),(\"F#\", 3),(\"G-\", 3),(\"G\", 2)])\n",
        "\n",
        "  for file in glob.glob(\"/content/drive/My Drive/Examination/MIDI/chorales/*.mid*\"):\n",
        "    print(\"Reading file №\" + str(k) + \"; \" + file)\n",
        "    \n",
        "    events = []\n",
        "\n",
        "    k= k + 1\n",
        "    main_offset = None\n",
        "\n",
        "    midi = converter.parse(file)\n",
        "    notes_to_parse = None\n",
        "    parts = instrument.partitionByInstrument(midi)\n",
        "\n",
        "    key = midi.analyze('key')\n",
        "    print(\"key before = \" + key.tonic.name, key.mode)\n",
        "\n",
        "    if key.mode == \"major\":\n",
        "        halfSteps = majors[key.tonic.name]\n",
        "\n",
        "    elif key.mode == \"minor\":\n",
        "        halfSteps = minors[key.tonic.name]\n",
        "\n",
        "    midi = midi.transpose(halfSteps)\n",
        "    key = midi.analyze('key')\n",
        "    print(\"key after  =  \" + key.tonic.name, key.mode)  \n",
        "\n",
        "    if parts: # Если есть хотя бы 1 партия\n",
        "        notes_to_parse = parts.parts[0].recurse()\n",
        "\n",
        "    else: # file has notes in a flat structure  (не понял для чего эта ветка)\n",
        "        notes_to_parse = midi.flat.notes\n",
        "\n",
        "    #<music21.meter.TimeSignature 4/4> -> 4/4\n",
        "    if midi.flat.timeSignature is not None :\n",
        "      signature = str(midi.flat.timeSignature).split(\" \")[1].split(\">\")[0].split(\"/\")[0]\n",
        "      signatures.append(signature)\n",
        "    else:\n",
        "      signatures.append(None)\n",
        "    \n",
        "    i = 1\n",
        "    for element in notes_to_parse:\n",
        "        if main_offset is None and (isinstance(element, note.Note) or isinstance(element, chord.Chord)):\n",
        "            if element.offset == 0:\n",
        "                main_offset = 0\n",
        "            else:\n",
        "                main_offset = element.offset\n",
        "\n",
        "        if isinstance(element, note.Note): #Записываем ноту\n",
        "            event = []\n",
        "            event.append(int((element.offset - main_offset) * 1000))\n",
        "            event.append(element.pitch.midi)\n",
        "            event.append(int(element.duration.quarterLength * 1000))\n",
        "            event.append(1)\n",
        "            event.append(element.volume.velocity)\n",
        "            events.append(event)\n",
        "\n",
        "        elif isinstance(element, chord.Chord):#Записываем аккорд\n",
        "            for e in element:\n",
        "                if isinstance(e, note.Note):\n",
        "                    event = []\n",
        "                    event.append(int((element.offset - main_offset) * 1000))\n",
        "                    event.append(e.pitch.midi)\n",
        "                    event.append(int(e.duration.quarterLength * 1000))\n",
        "                    event.append(1)\n",
        "                    event.append(e.volume.velocity)\n",
        "                    events.append(event)\n",
        "    \n",
        "    chorales.append(events)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file №1; /content/drive/My Drive/Examination/MIDI/chorales/04esistd.mid\n",
            "key before = E major\n",
            "key after  =  C major\n",
            "Reading file №2; /content/drive/My Drive/Examination/MIDI/chorales/10austie.mid\n",
            "key before = A minor\n",
            "key after  =  A minor\n",
            "Reading file №3; /content/drive/My Drive/Examination/MIDI/chorales/01ausmei.mid\n",
            "key before = G major\n",
            "key after  =  C major\n",
            "Reading file №4; /content/drive/My Drive/Examination/MIDI/chorales/03achgot.mid\n",
            "key before = A minor\n",
            "key after  =  A minor\n",
            "Reading file №5; /content/drive/My Drive/Examination/MIDI/chorales/02ichdan.mid\n",
            "key before = A major\n",
            "key after  =  C major\n",
            "Reading file №6; /content/drive/My Drive/Examination/MIDI/chorales/09ermunt.mid\n",
            "key before = G major\n",
            "key after  =  C major\n",
            "Reading file №7; /content/drive/My Drive/Examination/MIDI/chorales/12puerna.mid\n",
            "key before = A minor\n",
            "key after  =  A minor\n",
            "Reading file №8; /content/drive/My Drive/Examination/MIDI/chorales/06christ.mid\n",
            "key before = F major\n",
            "key after  =  C major\n",
            "Reading file №9; /content/drive/My Drive/Examination/MIDI/chorales/11jesu.mid\n",
            "key before = C major\n",
            "key after  =  C major\n",
            "Reading file №10; /content/drive/My Drive/Examination/MIDI/chorales/08freuet.mid\n",
            "key before = F minor\n",
            "key after  =  A minor\n",
            "Reading file №11; /content/drive/My Drive/Examination/MIDI/chorales/05anwass.mid\n",
            "key before = G major\n",
            "key after  =  C major\n",
            "Reading file №12; /content/drive/My Drive/Examination/MIDI/chorales/07nunlob.mid\n",
            "key before = A major\n",
            "key after  =  C major\n",
            "Reading file №13; /content/drive/My Drive/Examination/MIDI/chorales/13allein.mid\n",
            "key before = A minor\n",
            "key after  =  A minor\n",
            "Reading file №14; /content/drive/My Drive/Examination/MIDI/chorales/15christ.mid\n",
            "key before = D minor\n",
            "key after  =  A minor\n",
            "Reading file №15; /content/drive/My Drive/Examination/MIDI/chorales/14oherre.mid\n",
            "key before = G major\n",
            "key after  =  C major\n",
            "Reading file №16; /content/drive/My Drive/Examination/MIDI/chorales/chor016.mid\n",
            "key before = B minor\n",
            "key after  =  A minor\n",
            "Reading file №17; /content/drive/My Drive/Examination/MIDI/chorales/chor017.mid\n",
            "key before = E minor\n",
            "key after  =  A minor\n",
            "Reading file №18; /content/drive/My Drive/Examination/MIDI/chorales/chor020.mid\n",
            "key before = B minor\n",
            "key after  =  A minor\n",
            "Reading file №19; /content/drive/My Drive/Examination/MIDI/chorales/chor021.mid\n",
            "key before = A minor\n",
            "key after  =  A minor\n",
            "Reading file №20; /content/drive/My Drive/Examination/MIDI/chorales/chor022.mid\n",
            "key before = E- major\n",
            "key after  =  C major\n",
            "Reading file №21; /content/drive/My Drive/Examination/MIDI/chorales/chor023.mid\n",
            "key before = A minor\n",
            "key after  =  A minor\n",
            "Reading file №22; /content/drive/My Drive/Examination/MIDI/chorales/chor024.mid\n",
            "key before = D major\n",
            "key after  =  C major\n",
            "Reading file №23; /content/drive/My Drive/Examination/MIDI/chorales/chor025.mid\n",
            "key before = F minor\n",
            "key after  =  A minor\n",
            "Reading file №24; /content/drive/My Drive/Examination/MIDI/chorales/chor026.mid\n",
            "key before = F major\n",
            "key after  =  C major\n",
            "Reading file №25; /content/drive/My Drive/Examination/MIDI/chorales/chor027.mid\n",
            "key before = B- major\n",
            "key after  =  C major\n",
            "Reading file №26; /content/drive/My Drive/Examination/MIDI/chorales/chor028.mid\n",
            "key before = B minor\n",
            "key after  =  A minor\n",
            "Reading file №27; /content/drive/My Drive/Examination/MIDI/chorales/chor029.mid\n",
            "key before = G major\n",
            "key after  =  C major\n",
            "Reading file №28; /content/drive/My Drive/Examination/MIDI/chorales/chor030.mid\n",
            "key before = E minor\n",
            "key after  =  A minor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bgtnZn_m77_",
        "outputId": "1033587b-48c8-4303-b4b5-f43a6f6e23f1"
      },
      "source": [
        "print(chorales[0][:9])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 67, 1000, 1, 64], [0, 64, 1000, 1, 64], [0, 60, 1000, 1, 64], [0, 48, 1000, 1, 64], [1000, 67, 1000, 1, 64], [1000, 62, 500, 1, 64], [1000, 62, 1000, 1, 64], [1000, 47, 1000, 1, 64], [1500, 64, 500, 1, 64]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDebk8FjfC86"
      },
      "source": [
        "Для каждого хорала выполняется SPEAC-анализ, после чего в каждое событие каждого хорала добавляется 3 SPEAC-символа: foreground, middleground, background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfWZ00jfIPkd",
        "outputId": "93810acd-316e-472a-8bb4-a39673ff2de6"
      },
      "source": [
        "meters = []\n",
        "speac_analyzes = []\n",
        "\n",
        "for i in range(1, len(chorales) + 1):\n",
        "  events = chorales[i - 1]\n",
        "\n",
        "  meter = 3\n",
        "  while meter < 21:\n",
        "      try:\n",
        "        result = get_note_and_speac(events, meter)\n",
        "        speac_analyzes.append(result)\n",
        "        print(\"SPEAC-analysis for file №\", i)\n",
        "        meters.append(meter)\n",
        "        break\n",
        "\n",
        "      except Exception as e:\n",
        "        meter += 1\n",
        "      pass\n",
        "  print(\"------------------\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SPEAC-analysis for file № 1\n",
            "------------------\n",
            "SPEAC-analysis for file № 2\n",
            "------------------\n",
            "SPEAC-analysis for file № 3\n",
            "------------------\n",
            "SPEAC-analysis for file № 4\n",
            "------------------\n",
            "SPEAC-analysis for file № 5\n",
            "------------------\n",
            "SPEAC-analysis for file № 6\n",
            "------------------\n",
            "SPEAC-analysis for file № 7\n",
            "------------------\n",
            "SPEAC-analysis for file № 8\n",
            "------------------\n",
            "SPEAC-analysis for file № 9\n",
            "------------------\n",
            "SPEAC-analysis for file № 10\n",
            "------------------\n",
            "SPEAC-analysis for file № 11\n",
            "------------------\n",
            "SPEAC-analysis for file № 12\n",
            "------------------\n",
            "SPEAC-analysis for file № 13\n",
            "------------------\n",
            "SPEAC-analysis for file № 14\n",
            "------------------\n",
            "SPEAC-analysis for file № 15\n",
            "------------------\n",
            "SPEAC-analysis for file № 16\n",
            "------------------\n",
            "SPEAC-analysis for file № 17\n",
            "------------------\n",
            "SPEAC-analysis for file № 18\n",
            "------------------\n",
            "SPEAC-analysis for file № 19\n",
            "------------------\n",
            "SPEAC-analysis for file № 20\n",
            "------------------\n",
            "SPEAC-analysis for file № 21\n",
            "------------------\n",
            "SPEAC-analysis for file № 22\n",
            "------------------\n",
            "SPEAC-analysis for file № 23\n",
            "------------------\n",
            "SPEAC-analysis for file № 24\n",
            "------------------\n",
            "SPEAC-analysis for file № 25\n",
            "------------------\n",
            "SPEAC-analysis for file № 26\n",
            "------------------\n",
            "SPEAC-analysis for file № 27\n",
            "------------------\n",
            "SPEAC-analysis for file № 28\n",
            "------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-6eZ6-ffY0L"
      },
      "source": [
        "События последовательно записываются друг за другом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5SUGNnEIStX",
        "outputId": "a2f392b7-ec4d-4c8c-e513-493a620d53e9"
      },
      "source": [
        "one_big_input = []\n",
        "big_offset = 0\n",
        "\n",
        "for speac_analysis in speac_analyzes:\n",
        "    for event in speac_analysis:\n",
        "            new_event = copy.deepcopy(event)\n",
        "            new_event[0] += big_offset\n",
        "            one_big_input.append(new_event)\n",
        "    \n",
        "    big_offset += speac_analysis[-1][0] + speac_analysis[-1][2]\n",
        "\n",
        "print(one_big_input[:9])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 48, 1000, 1, 64, 'statement', 'preparation', 'statement'], [0, 60, 1000, 1, 64, 'statement', 'preparation', 'statement'], [0, 64, 1000, 1, 64, 'statement', 'preparation', 'statement'], [0, 67, 1000, 1, 64, 'statement', 'preparation', 'statement'], [1000, 47, 500, 1, 64, 'extension', 'preparation', 'statement'], [1000, 62, 500, 1, 64, 'extension', 'preparation', 'statement'], [1000, 62, 500, 1, 64, 'extension', 'preparation', 'statement'], [1000, 67, 500, 1, 64, 'extension', 'preparation', 'statement'], [1500, 47, 500, 1, 64, 'extension', 'preparation', 'statement']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z8Q0xsYf7EE"
      },
      "source": [
        "Кодирование каждого бита (одновременно играющихся событий) в формате: \"midi.midi.midi\", предложенном в статье https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlmKl8SiOv_C",
        "outputId": "f2337f55-e874-4859-b79a-4da7c534ef00"
      },
      "source": [
        "notes_article = []\n",
        "current_chord = None\n",
        "\n",
        "last_event_offset = None\n",
        "\n",
        "for event in one_big_input:\n",
        "    event_offset = event[0]\n",
        "    \n",
        "    if last_event_offset is None:\n",
        "        current_chord = str(event[1])\n",
        "    \n",
        "    elif event_offset > last_event_offset:\n",
        "        notes_article.append(current_chord)\n",
        "        current_chord = str(event[1])\n",
        "\n",
        "    else:\n",
        "        current_chord = current_chord + \".\" + str(event[1])\n",
        "\n",
        "    last_event_offset = event_offset\n",
        "    \n",
        "print(notes_article[:9])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['48.60.64.67', '47.62.62.67', '47.62.64.67', '43.60.65.67', '43.59.65.67', '48.60.64.67', '50.53.62.70', '52.55.62.70', '53.57.60.69']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLJ5IZvmVF0C"
      },
      "source": [
        "Если после установки что-то в коде не работает, то есть вероятность, что надо сбросить настройки в среде выполнения или изменить её на TPU соответственно)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhdgo6qGSaRV",
        "outputId": "c6e408f2-8b10-43a2-9719-1b0138eb9f47"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.88.227.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.88.227.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.88.227.18:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.88.227.18:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9VdV8aWVPJL"
      },
      "source": [
        "TPU имеет 8 логических устройств TPU (0–7), которые могут выполнять параллельную обработку. Следовательно, мы определяем стратегию распределения для распределенного обучения по этим 8 устройствам:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2NjrCByVMLn",
        "outputId": "5bc4cf5f-6ca2-4221-f065-37e24cf43cb8"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqGwFxJFgqLN"
      },
      "source": [
        "Подготовка данных для обучения модели из статьи"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVuaaKafO3-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522e06c8-0933-485d-9f1f-6054931fd05c"
      },
      "source": [
        "#Минимальное необходимое число нот в обучающих произведениях\n",
        "sequence_length = 8\n",
        "\n",
        "#Сортировка всех повторяющихся нот в множество неповторяющихся (Set)\n",
        "pitchnames_article = sorted(set(item for item in notes_article))\n",
        "\n",
        "#Отображаем каждый аккорд или ноту на некоторое число (по сути Map)\n",
        "note_to_int_article = dict((note, number) for number, note in enumerate(pitchnames_article))\n",
        "\n",
        "network_input_article = []\n",
        "network_output_article = []\n",
        "\n",
        "for i in range(0, len(notes_article) - sequence_length, 1):\n",
        "    #записываем сначала первые 100 нот, затем 1-101, 2-102 и т.д\n",
        "    sequence_in_article = notes_article[i:i + sequence_length]\n",
        "\n",
        "    #узнаём какой должна быть следующая нота\n",
        "    sequence_out_article = notes_article[i + sequence_length]\n",
        "\n",
        "    #ноты преобразуем в номера из созданного Map\n",
        "    network_input_article.append([note_to_int_article[char] for char in sequence_in_article])\n",
        "\n",
        "    #преобразуем следующую ноту в номер из созданного Map\n",
        "    network_output_article.append(note_to_int_article[sequence_out_article])\n",
        "\n",
        "n_patterns_article = len(network_input_article)\n",
        "\n",
        "#Преобразование входной выборки в формат, удобный для работы с LSTM\n",
        "network_input_article = numpy.reshape(network_input_article, (n_patterns_article, sequence_length, 1))\n",
        "\n",
        "n_vocab_article = len(set(notes_article))\n",
        "# n_vocab = max(set(network_output)) + 1\n",
        "\n",
        "#Нормализация\n",
        "network_input_article = network_input_article / float(n_vocab_article)\n",
        "\n",
        "network_output_article = np_utils.to_categorical(network_output_article)\n",
        "\n",
        "print(network_input_article[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.26683292]\n",
            " [0.21030756]\n",
            " [0.21113882]\n",
            " [0.08312552]\n",
            " [0.07813799]\n",
            " [0.26683292]\n",
            " [0.32751455]\n",
            " [0.46716542]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpLWN-8cVSWY"
      },
      "source": [
        "def create_model(network_input, n_vocab):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(\n",
        "        256,\n",
        "        input_shape=(network_input.shape[1], \n",
        "                     network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(512, return_sequences=True))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(256))\n",
        "  model.add(Dense(256))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv5mjVrIgvpy"
      },
      "source": [
        "Компиляция модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U-PSU1uVgz5",
        "outputId": "802377cc-e102-472f-8702-9b14b5c28e0a"
      },
      "source": [
        "with strategy.scope():\n",
        "    model_article = create_model(network_input_article, n_vocab_article)\n",
        "    model_article.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "    print(\"model_article is compilled\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 8, 256)            264192    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 512)            1574912   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               787456    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1203)              309171    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1203)              0         \n",
            "=================================================================\n",
            "Total params: 3,001,523\n",
            "Trainable params: 3,001,523\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model_article is compilled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezoQNB6PgxuA"
      },
      "source": [
        "Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa_jqiVHWBZb",
        "outputId": "21e51483-f734-494f-8914-b6790ac59b83"
      },
      "source": [
        "model_article.fit(network_input_article, network_output_article, epochs=200, batch_size=64)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "43/43 [==============================] - 29s 79ms/step - loss: 7.0183\n",
            "Epoch 2/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.7539\n",
            "Epoch 3/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.7263\n",
            "Epoch 4/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.6173\n",
            "Epoch 5/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.6102\n",
            "Epoch 6/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 6.5597\n",
            "Epoch 7/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.4912\n",
            "Epoch 8/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.5048\n",
            "Epoch 9/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.4285\n",
            "Epoch 10/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.4167\n",
            "Epoch 11/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.4099\n",
            "Epoch 12/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.3338\n",
            "Epoch 13/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.2777\n",
            "Epoch 14/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.2727\n",
            "Epoch 15/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.2103\n",
            "Epoch 16/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.2051\n",
            "Epoch 17/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.0748\n",
            "Epoch 18/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 6.1038\n",
            "Epoch 19/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 5.9767\n",
            "Epoch 20/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.9056\n",
            "Epoch 21/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.8705\n",
            "Epoch 22/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 5.7609\n",
            "Epoch 23/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 5.7610\n",
            "Epoch 24/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 5.7178\n",
            "Epoch 25/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.6393\n",
            "Epoch 26/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.5968\n",
            "Epoch 27/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.5145\n",
            "Epoch 28/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 5.4479\n",
            "Epoch 29/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.4380\n",
            "Epoch 30/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 5.3738\n",
            "Epoch 31/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.3044\n",
            "Epoch 32/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.2275\n",
            "Epoch 33/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.1171\n",
            "Epoch 34/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.0162\n",
            "Epoch 35/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5.0094\n",
            "Epoch 36/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.9409\n",
            "Epoch 37/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.8526\n",
            "Epoch 38/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 4.7515\n",
            "Epoch 39/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.7258\n",
            "Epoch 40/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.7069\n",
            "Epoch 41/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 4.5656\n",
            "Epoch 42/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.4953\n",
            "Epoch 43/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.4928\n",
            "Epoch 44/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.3907\n",
            "Epoch 45/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.4197\n",
            "Epoch 46/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.3111\n",
            "Epoch 47/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.1568\n",
            "Epoch 48/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.1851\n",
            "Epoch 49/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 4.0749\n",
            "Epoch 50/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.9428\n",
            "Epoch 51/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.9752\n",
            "Epoch 52/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 3.8158\n",
            "Epoch 53/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.7520\n",
            "Epoch 54/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.7749\n",
            "Epoch 55/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.6543\n",
            "Epoch 56/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 3.6793\n",
            "Epoch 57/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.6537\n",
            "Epoch 58/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.3791\n",
            "Epoch 59/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.4360\n",
            "Epoch 60/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.3860\n",
            "Epoch 61/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.2893\n",
            "Epoch 62/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.2851\n",
            "Epoch 63/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.1665\n",
            "Epoch 64/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.1164\n",
            "Epoch 65/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 3.0058\n",
            "Epoch 66/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.9483\n",
            "Epoch 67/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.9126\n",
            "Epoch 68/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.8822\n",
            "Epoch 69/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 2.8328\n",
            "Epoch 70/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.7865\n",
            "Epoch 71/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.7249\n",
            "Epoch 72/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.6175\n",
            "Epoch 73/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.5511\n",
            "Epoch 74/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.6155\n",
            "Epoch 75/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.5606\n",
            "Epoch 76/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.4502\n",
            "Epoch 77/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.4165\n",
            "Epoch 78/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.4114\n",
            "Epoch 79/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.3185\n",
            "Epoch 80/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.3001\n",
            "Epoch 81/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.3329\n",
            "Epoch 82/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.2885\n",
            "Epoch 83/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 2.2111\n",
            "Epoch 84/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.1137\n",
            "Epoch 85/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 2.1219\n",
            "Epoch 86/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.0019\n",
            "Epoch 87/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.1163\n",
            "Epoch 88/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 2.0407\n",
            "Epoch 89/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.9651\n",
            "Epoch 90/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.8949\n",
            "Epoch 91/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.9862\n",
            "Epoch 92/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.8680\n",
            "Epoch 93/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.8347\n",
            "Epoch 94/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.8798\n",
            "Epoch 95/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.7643\n",
            "Epoch 96/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.7126\n",
            "Epoch 97/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.6732\n",
            "Epoch 98/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.6324\n",
            "Epoch 99/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.7167\n",
            "Epoch 100/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.6786\n",
            "Epoch 101/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.6475\n",
            "Epoch 102/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.5679\n",
            "Epoch 103/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.5726\n",
            "Epoch 104/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.4698\n",
            "Epoch 105/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.4496\n",
            "Epoch 106/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.4519\n",
            "Epoch 107/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.4936\n",
            "Epoch 108/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.4087\n",
            "Epoch 109/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.3972\n",
            "Epoch 110/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.4025\n",
            "Epoch 111/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.3054\n",
            "Epoch 112/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.3369\n",
            "Epoch 113/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.2511\n",
            "Epoch 114/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.2488\n",
            "Epoch 115/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.2605\n",
            "Epoch 116/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.2517\n",
            "Epoch 117/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.1365\n",
            "Epoch 118/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.1876\n",
            "Epoch 119/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.1917\n",
            "Epoch 120/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.2466\n",
            "Epoch 121/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.1660\n",
            "Epoch 122/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.1075\n",
            "Epoch 123/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.0893\n",
            "Epoch 124/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1.0889\n",
            "Epoch 125/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 1.1365\n",
            "Epoch 126/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.9728\n",
            "Epoch 127/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.0346\n",
            "Epoch 128/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.9924\n",
            "Epoch 129/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 1.1006\n",
            "Epoch 130/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 1.0040\n",
            "Epoch 131/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.9290\n",
            "Epoch 132/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.9491\n",
            "Epoch 133/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 0.9545\n",
            "Epoch 134/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.9920\n",
            "Epoch 135/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.9481\n",
            "Epoch 136/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.9084\n",
            "Epoch 137/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.9086\n",
            "Epoch 138/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.8633\n",
            "Epoch 139/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.8874\n",
            "Epoch 140/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.8182\n",
            "Epoch 141/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.8573\n",
            "Epoch 142/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.8128\n",
            "Epoch 143/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.8016\n",
            "Epoch 144/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.8046\n",
            "Epoch 145/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.8123\n",
            "Epoch 146/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.7633\n",
            "Epoch 147/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.7716\n",
            "Epoch 148/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.7301\n",
            "Epoch 149/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.7273\n",
            "Epoch 150/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.7966\n",
            "Epoch 151/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.7801\n",
            "Epoch 152/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.7525\n",
            "Epoch 153/200\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.6945\n",
            "Epoch 154/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.6693\n",
            "Epoch 155/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.6828\n",
            "Epoch 156/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.7562\n",
            "Epoch 157/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.6988\n",
            "Epoch 158/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.6494\n",
            "Epoch 159/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.6948\n",
            "Epoch 160/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.6919\n",
            "Epoch 161/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 0.6483\n",
            "Epoch 162/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 0.5985\n",
            "Epoch 163/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.6156\n",
            "Epoch 164/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.6343\n",
            "Epoch 165/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 0.5900\n",
            "Epoch 166/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 0.6412\n",
            "Epoch 167/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.6399\n",
            "Epoch 168/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5701\n",
            "Epoch 169/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.5499\n",
            "Epoch 170/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.5941\n",
            "Epoch 171/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5863\n",
            "Epoch 172/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.5723\n",
            "Epoch 173/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.6011\n",
            "Epoch 174/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5601\n",
            "Epoch 175/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5587\n",
            "Epoch 176/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5789\n",
            "Epoch 177/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5655\n",
            "Epoch 178/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.6061\n",
            "Epoch 179/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5800\n",
            "Epoch 180/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5664\n",
            "Epoch 181/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4986\n",
            "Epoch 182/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5289\n",
            "Epoch 183/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5142\n",
            "Epoch 184/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5125\n",
            "Epoch 185/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4830\n",
            "Epoch 186/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.5263\n",
            "Epoch 187/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5074\n",
            "Epoch 188/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.4908\n",
            "Epoch 189/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.4937\n",
            "Epoch 190/200\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.4773\n",
            "Epoch 191/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4937\n",
            "Epoch 192/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4619\n",
            "Epoch 193/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4845\n",
            "Epoch 194/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4641\n",
            "Epoch 195/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.5021\n",
            "Epoch 196/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4821\n",
            "Epoch 197/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4635\n",
            "Epoch 198/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4834\n",
            "Epoch 199/200\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.4586\n",
            "Epoch 200/200\n",
            "43/43 [==============================] - 1s 18ms/step - loss: 0.4486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9740f04510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRNJeg8CWJoi"
      },
      "source": [
        "model_article.save_weights('/content/drive/My Drive/Examination/Weights/model_article_weights.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvHNaR9Jg1UK"
      },
      "source": [
        "Генерация следующих битов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Pof4twW2i2",
        "outputId": "a06b0f65-e6cb-47c9-eba4-6f32edfa586c"
      },
      "source": [
        "  # start = numpy.random.randint(0, len(network_input_article)-1)\n",
        "  start = 0\n",
        "  int_to_note_article = dict((number, note) for number, note in enumerate(pitchnames_article))\n",
        "\n",
        "  pattern_article = network_input_article[start]\n",
        "\n",
        "  prediction_output_article = []\n",
        "\n",
        "  # generate n notes\n",
        "  for note_index in range(50):\n",
        "        print(\"generating \", note_index, \" note\")\n",
        "        prediction_input_article = numpy.reshape(pattern_article, (1, len(pattern_article), 1))\n",
        "        prediction_input_article = prediction_input_article / float(n_vocab_article)\n",
        "\n",
        "        prediction_article = model_article.predict(prediction_input_article, verbose=0)\n",
        "\n",
        "        index_article = numpy.argmax(prediction_article)\n",
        "        result_article = int_to_note_article[index_article]\n",
        "        prediction_output_article.append(result_article)\n",
        "\n",
        "        pattern_article = numpy.append(pattern_article, [index_article])\n",
        "        pattern_article = pattern_article[1:len(pattern_article)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating  0  note\n",
            "generating  1  note\n",
            "generating  2  note\n",
            "generating  3  note\n",
            "generating  4  note\n",
            "generating  5  note\n",
            "generating  6  note\n",
            "generating  7  note\n",
            "generating  8  note\n",
            "generating  9  note\n",
            "generating  10  note\n",
            "generating  11  note\n",
            "generating  12  note\n",
            "generating  13  note\n",
            "generating  14  note\n",
            "generating  15  note\n",
            "generating  16  note\n",
            "generating  17  note\n",
            "generating  18  note\n",
            "generating  19  note\n",
            "generating  20  note\n",
            "generating  21  note\n",
            "generating  22  note\n",
            "generating  23  note\n",
            "generating  24  note\n",
            "generating  25  note\n",
            "generating  26  note\n",
            "generating  27  note\n",
            "generating  28  note\n",
            "generating  29  note\n",
            "generating  30  note\n",
            "generating  31  note\n",
            "generating  32  note\n",
            "generating  33  note\n",
            "generating  34  note\n",
            "generating  35  note\n",
            "generating  36  note\n",
            "generating  37  note\n",
            "generating  38  note\n",
            "generating  39  note\n",
            "generating  40  note\n",
            "generating  41  note\n",
            "generating  42  note\n",
            "generating  43  note\n",
            "generating  44  note\n",
            "generating  45  note\n",
            "generating  46  note\n",
            "generating  47  note\n",
            "generating  48  note\n",
            "generating  49  note\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIYs4ZdeCrzx"
      },
      "source": [
        "import music21\n",
        "from music21 import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COHA_bGIc3S4"
      },
      "source": [
        "def create_new_midi(predicted, file_name):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "\n",
        "    for pattern in predicted:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            note_index = 0\n",
        "\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "                    \n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        offset += 1.0\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    file = midi_stream.write('midi', fp=file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVc4qCYWRocu"
      },
      "source": [
        "create_new_midi(prediction_output_article, \"article_n=\" + str(sequence_length) + \".mid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GurIFR7ff4c6"
      },
      "source": [
        "Попытаемся расширить входной вектор так, чтобы на вход нейросети подавался не скаляр, а вектор из 2-х переменных: аккорды и спеак. Будем пытаться повторять эксперимент, который проводил Коуп, когда генерировал музыку. То есть он на основе одного аккорда и желаемого следующего SPEAC-состояния по определённому алгоритму генерировал следующий аккорд. Мы делаем то же самое, но вместо алгоритма Коупа будем использовать нейронную сеть.\n",
        "Здесь стоит учесть, что при кодировке всех трёх символов одним разница между foreground состояниями должна быть минимальной, то есть нужно правильно отсортировать данные.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sskM09leaHEm",
        "outputId": "5349b14e-535b-432d-b241-5d73c7bb0c1d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.88.227.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.88.227.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.88.227.18:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.88.227.18:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWc-cTFlaMFm",
        "outputId": "e60e85be-a921-4999-8126-fd4262ff39b7"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD4-oBUFhN-O"
      },
      "source": [
        "Сортировка SPEAC-состояний по мере напряженности аккорда "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xY5ofXFim-7"
      },
      "source": [
        "speac_symbols = [\"antecedent\", \"preparation\", \"extension\", \"statement\", \"consequent\"]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbhqKSWYA_Jg"
      },
      "source": [
        "speac_map_number = 1\n",
        "sorted_speac_to_int = {}\n",
        "\n",
        "for background in speac_symbols:\n",
        "\n",
        "    for middleground in speac_symbols:\n",
        "\n",
        "        for foreground in speac_symbols:\n",
        "            speac_string = str(foreground) + \"-\" + str(middleground) + \"-\" + str(background)\n",
        "            sorted_speac_to_int[speac_string] = speac_map_number\n",
        "            speac_map_number += 1   "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNHfWvo1hVRn"
      },
      "source": [
        "Подготовка данных для обучения "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSPwz5CKjOB9",
        "outputId": "eb5835dd-a775-4db4-878a-bdab01cb3af5"
      },
      "source": [
        "notes = []\n",
        "note = []\n",
        "one_time_notes_set = set()\n",
        "\n",
        "last_offset = None\n",
        "one_time_notes = None\n",
        "\n",
        "for event in one_big_input:\n",
        "    if last_offset is None:\n",
        "        one_time_notes = str(event[1])\n",
        "    \n",
        "    elif event[0] == last_offset:\n",
        "        one_time_notes = one_time_notes + \".\" + str(event[1])\n",
        "    \n",
        "    else:\n",
        "        next_speac = str(event[5]) + \"-\" + str(event[6]) + \"-\" + str(event[7])\n",
        "        note = [one_time_notes, next_speac]\n",
        "        one_time_notes_set.add(one_time_notes)\n",
        "        notes.append(note)\n",
        "        one_time_notes = str(event[1])\n",
        "\n",
        "    last_offset = event[0]\n",
        "\n",
        "print(notes[:9])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['48.60.64.67', 'extension-preparation-statement'], ['47.62.62.67', 'extension-preparation-statement'], ['47.62.64.67', 'antecedent-preparation-statement'], ['43.60.65.67', 'antecedent-preparation-statement'], ['43.59.65.67', 'statement-preparation-statement'], ['48.60.64.67', 'preparation-preparation-statement'], ['50.53.62.70', 'preparation-preparation-statement'], ['52.55.62.70', 'extension-preparation-statement'], ['53.57.60.69', 'extension-preparation-statement']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puqWEcOof1TG",
        "outputId": "8f111016-6955-47e8-bf09-272ae4be85a6"
      },
      "source": [
        "#Минимальное необходимое число нот в обучающих произведениях\n",
        "sequence_length = 38\n",
        "sequence_length += 1\n",
        "\n",
        "#Сортировка всех повторяющихся нот в множество неповторяющихся (Set)\n",
        "notes_pitchnames = sorted(set(item for item in one_time_notes_set))\n",
        "\n",
        "#Отображаем каждый аккорд или ноту на некоторое число (по сути Map)\n",
        "notes_to_int = dict((note, number) for number, note in enumerate(notes_pitchnames))\n",
        "\n",
        "network_input = []\n",
        "network_input_without_normalisation = []\n",
        "network_output = []\n",
        "\n",
        "n_vocab_notes = len(one_time_notes_set)\n",
        "n_vocab_speac = len(sorted_speac_to_int)\n",
        "\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "    sequence_in = notes[i:i + sequence_length]\n",
        "\n",
        "    # Берём только ноты, без speac\n",
        "    sequence_out_notes = notes[i + sequence_length][0]  \n",
        "\n",
        "    local_network_input = []\n",
        "    local_network_input_without_normalisation = []\n",
        "    for j in range(1, sequence_length):\n",
        "        notes_in_input = sequence_in[j - 1][0]\n",
        "        speac_in_input = sequence_in[j][1]      # Подаём не текущий speac, а следующий\n",
        "\n",
        "        notes_to_number = notes_to_int[notes_in_input]\n",
        "        speac_to_number = sorted_speac_to_int[speac_in_input]\n",
        "\n",
        "        normalized_notes_to_number = notes_to_number / float(n_vocab_notes)  \n",
        "        normalized_speac_to_number = speac_to_number / float(n_vocab_speac) \n",
        "\n",
        "        local_network_input.append([normalized_notes_to_number, normalized_speac_to_number])\n",
        "        local_network_input_without_normalisation.append([notes_to_number, normalized_speac_to_number])\n",
        "\n",
        "    network_input.append(local_network_input)\n",
        "    network_input_without_normalisation.append(local_network_input_without_normalisation)\n",
        "\n",
        "    network_output.append(notes_to_int[sequence_out_notes])\n",
        "\n",
        "print(\"normalized network input[0]:\")\n",
        "print(network_input[0])\n",
        "print(\"----------------\")\n",
        "\n",
        "n_patterns = len(network_input)\n",
        "sequence_length -= 1\n",
        "\n",
        "#Преобразование входной выборки в формат, удобный для работы с LSTM\n",
        "network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 2))  # подавать в нейросеть данные по 2 элемента\n",
        "print(\"reshaped network input[0]:\")\n",
        "print(network_input[0])\n",
        "print(\"----------------\")\n",
        "\n",
        "network_output = np_utils.to_categorical(network_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalized network input[0]:\n",
            "[[0.2666112956810631, 0.7407407407407407], [0.2101328903654485, 0.7314814814814815], [0.21096345514950166, 0.7314814814814815], [0.08305647840531562, 0.7453703703703703], [0.07807308970099668, 0.7361111111111112], [0.2666112956810631, 0.7361111111111112], [0.3272425249169435, 0.7407407407407407], [0.46677740863787376, 0.7407407407407407], [0.574750830564784, 0.7407407407407407], [0.5872093023255814, 0.7407407407407407], [0.5980066445182725, 0.7407407407407407], [0.2707641196013289, 0.7453703703703703], [0.2599667774086379, 0.7314814814814815], [0.04152823920265781, 0.75], [0.19186046511627908, 0.75], [0.24086378737541528, 0.7453703703703703], [0.33637873754152825, 0.7453703703703703], [0.4601328903654485, 0.7407407407407407], [0.45598006644518274, 0.7407407407407407], [0.10382059800664452, 0.7407407407407407], [0.32475083056478404, 0.7361111111111112], [0.23338870431893688, 0.7361111111111112], [0.17857142857142858, 0.7361111111111112], [0.11129568106312292, 0.7361111111111112], [0.061461794019933555, 0.7361111111111112], [0.3372093023255814, 0.7407407407407407], [0.3313953488372093, 0.7453703703703703], [0.05398671096345515, 0.7407407407407407], [0.2666112956810631, 0.7407407407407407], [0.2101328903654485, 0.7314814814814815], [0.21096345514950166, 0.7314814814814815], [0.08305647840531562, 0.7453703703703703], [0.07807308970099668, 0.7361111111111112], [0.2666112956810631, 0.7361111111111112], [0.3272425249169435, 0.7407407407407407], [0.46677740863787376, 0.7407407407407407], [0.574750830564784, 0.7407407407407407], [0.5872093023255814, 0.7407407407407407]]\n",
            "----------------\n",
            "reshaped network input[0]:\n",
            "[[0.2666113  0.74074074]\n",
            " [0.21013289 0.73148148]\n",
            " [0.21096346 0.73148148]\n",
            " [0.08305648 0.74537037]\n",
            " [0.07807309 0.73611111]\n",
            " [0.2666113  0.73611111]\n",
            " [0.32724252 0.74074074]\n",
            " [0.46677741 0.74074074]\n",
            " [0.57475083 0.74074074]\n",
            " [0.5872093  0.74074074]\n",
            " [0.59800664 0.74074074]\n",
            " [0.27076412 0.74537037]\n",
            " [0.25996678 0.73148148]\n",
            " [0.04152824 0.75      ]\n",
            " [0.19186047 0.75      ]\n",
            " [0.24086379 0.74537037]\n",
            " [0.33637874 0.74537037]\n",
            " [0.46013289 0.74074074]\n",
            " [0.45598007 0.74074074]\n",
            " [0.1038206  0.74074074]\n",
            " [0.32475083 0.73611111]\n",
            " [0.2333887  0.73611111]\n",
            " [0.17857143 0.73611111]\n",
            " [0.11129568 0.73611111]\n",
            " [0.06146179 0.73611111]\n",
            " [0.3372093  0.74074074]\n",
            " [0.33139535 0.74537037]\n",
            " [0.05398671 0.74074074]\n",
            " [0.2666113  0.74074074]\n",
            " [0.21013289 0.73148148]\n",
            " [0.21096346 0.73148148]\n",
            " [0.08305648 0.74537037]\n",
            " [0.07807309 0.73611111]\n",
            " [0.2666113  0.73611111]\n",
            " [0.32724252 0.74074074]\n",
            " [0.46677741 0.74074074]\n",
            " [0.57475083 0.74074074]\n",
            " [0.5872093  0.74074074]]\n",
            "----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlIrLnm3aVk2"
      },
      "source": [
        "def create_new_model(network_input, n_vocab):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(\n",
        "        256,\n",
        "        input_shape=(network_input.shape[1], \n",
        "                     network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(512, return_sequences=True))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(256))\n",
        "  model.add(Dense(256))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqBAorzBaVk7",
        "outputId": "a27962da-0fc8-4966-c60e-fdab61f9b7f2"
      },
      "source": [
        "with strategy.scope():\n",
        "    model_only_notes = create_new_model(network_input, n_vocab_notes)\n",
        "    model_only_notes.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "    print(\"model_only_notes is compilled\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 38, 256)           265216    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 38, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 38, 512)           1574912   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 38, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 256)               787456    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1204)              309428    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1204)              0         \n",
            "=================================================================\n",
            "Total params: 3,002,804\n",
            "Trainable params: 3,002,804\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model_only_notes is compilled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOhZZ4pqaVk8",
        "outputId": "3efa1a70-9e73-4ded-dd78-f7bd78795435"
      },
      "source": [
        "model_only_notes.fit(network_input, network_output, epochs=200, batch_size=64)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "43/43 [==============================] - 13s 92ms/step - loss: 7.0475\n",
            "Epoch 2/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.7892\n",
            "Epoch 3/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.7151\n",
            "Epoch 4/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.6115\n",
            "Epoch 5/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.5541\n",
            "Epoch 6/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.5010\n",
            "Epoch 7/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.4712\n",
            "Epoch 8/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.4187\n",
            "Epoch 9/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.2985\n",
            "Epoch 10/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 6.2907\n",
            "Epoch 11/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.2353\n",
            "Epoch 12/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.0816\n",
            "Epoch 13/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 6.0342\n",
            "Epoch 14/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 5.8881\n",
            "Epoch 15/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 5.7880\n",
            "Epoch 16/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 5.6817\n",
            "Epoch 17/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 5.5357\n",
            "Epoch 18/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 5.4243\n",
            "Epoch 19/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 5.2971\n",
            "Epoch 20/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 5.1592\n",
            "Epoch 21/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 4.9974\n",
            "Epoch 22/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 4.8510\n",
            "Epoch 23/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 4.6916\n",
            "Epoch 24/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 4.6167\n",
            "Epoch 25/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 4.4700\n",
            "Epoch 26/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 4.2982\n",
            "Epoch 27/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 4.1341\n",
            "Epoch 28/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 3.9882\n",
            "Epoch 29/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 3.8795\n",
            "Epoch 30/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 3.7881\n",
            "Epoch 31/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 3.6344\n",
            "Epoch 32/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 3.5628\n",
            "Epoch 33/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 3.3779\n",
            "Epoch 34/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 3.3422\n",
            "Epoch 35/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 3.2339\n",
            "Epoch 36/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 3.0908\n",
            "Epoch 37/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.9614\n",
            "Epoch 38/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.9381\n",
            "Epoch 39/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.8368\n",
            "Epoch 40/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.7452\n",
            "Epoch 41/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.7499\n",
            "Epoch 42/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 2.5945\n",
            "Epoch 43/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.5172\n",
            "Epoch 44/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.4652\n",
            "Epoch 45/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.4109\n",
            "Epoch 46/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.3863\n",
            "Epoch 47/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.3075\n",
            "Epoch 48/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.3098\n",
            "Epoch 49/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 2.1331\n",
            "Epoch 50/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.1277\n",
            "Epoch 51/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 2.1245\n",
            "Epoch 52/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.0456\n",
            "Epoch 53/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 2.0748\n",
            "Epoch 54/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.9616\n",
            "Epoch 55/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 1.9488\n",
            "Epoch 56/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.8781\n",
            "Epoch 57/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.8895\n",
            "Epoch 58/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.8064\n",
            "Epoch 59/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.7710\n",
            "Epoch 60/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.7495\n",
            "Epoch 61/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.6986\n",
            "Epoch 62/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.6578\n",
            "Epoch 63/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.6087\n",
            "Epoch 64/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 1.5985\n",
            "Epoch 65/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.5306\n",
            "Epoch 66/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.4971\n",
            "Epoch 67/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.4570\n",
            "Epoch 68/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.4692\n",
            "Epoch 69/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.4319\n",
            "Epoch 70/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.3930\n",
            "Epoch 71/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.3759\n",
            "Epoch 72/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.3416\n",
            "Epoch 73/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.3514\n",
            "Epoch 74/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.2966\n",
            "Epoch 75/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 1.2343\n",
            "Epoch 76/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 1.2278\n",
            "Epoch 77/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.2107\n",
            "Epoch 78/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 1.2080\n",
            "Epoch 79/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.1811\n",
            "Epoch 80/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 1.1295\n",
            "Epoch 81/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.1341\n",
            "Epoch 82/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.0817\n",
            "Epoch 83/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 1.1003\n",
            "Epoch 84/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.0043\n",
            "Epoch 85/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.0067\n",
            "Epoch 86/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 1.0444\n",
            "Epoch 87/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 0.9954\n",
            "Epoch 88/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.9249\n",
            "Epoch 89/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.9458\n",
            "Epoch 90/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.9121\n",
            "Epoch 91/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.8732\n",
            "Epoch 92/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.8708\n",
            "Epoch 93/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.8444\n",
            "Epoch 94/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.8401\n",
            "Epoch 95/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.8090\n",
            "Epoch 96/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.8160\n",
            "Epoch 97/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.7516\n",
            "Epoch 98/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.7601\n",
            "Epoch 99/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.7601\n",
            "Epoch 100/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.7383\n",
            "Epoch 101/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.7277\n",
            "Epoch 102/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.7316\n",
            "Epoch 103/200\n",
            "43/43 [==============================] - 1s 29ms/step - loss: 0.7104\n",
            "Epoch 104/200\n",
            "43/43 [==============================] - 1s 30ms/step - loss: 0.6736\n",
            "Epoch 105/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.7127\n",
            "Epoch 106/200\n",
            "43/43 [==============================] - 1s 29ms/step - loss: 0.6029\n",
            "Epoch 107/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.6657\n",
            "Epoch 108/200\n",
            "43/43 [==============================] - 1s 29ms/step - loss: 0.6113\n",
            "Epoch 109/200\n",
            "43/43 [==============================] - 1s 29ms/step - loss: 0.5845\n",
            "Epoch 110/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5921\n",
            "Epoch 111/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5830\n",
            "Epoch 112/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5840\n",
            "Epoch 113/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5747\n",
            "Epoch 114/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.5617\n",
            "Epoch 115/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5117\n",
            "Epoch 116/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5205\n",
            "Epoch 117/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.5217\n",
            "Epoch 118/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4669\n",
            "Epoch 119/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5145\n",
            "Epoch 120/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4601\n",
            "Epoch 121/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4629\n",
            "Epoch 122/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4777\n",
            "Epoch 123/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5048\n",
            "Epoch 124/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4161\n",
            "Epoch 125/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4929\n",
            "Epoch 126/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.4447\n",
            "Epoch 127/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4514\n",
            "Epoch 128/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4405\n",
            "Epoch 129/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4174\n",
            "Epoch 130/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.4313\n",
            "Epoch 131/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3729\n",
            "Epoch 132/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3942\n",
            "Epoch 133/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3762\n",
            "Epoch 134/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3700\n",
            "Epoch 135/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3904\n",
            "Epoch 136/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3689\n",
            "Epoch 137/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3674\n",
            "Epoch 138/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3531\n",
            "Epoch 139/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3466\n",
            "Epoch 140/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3440\n",
            "Epoch 141/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3110\n",
            "Epoch 142/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3527\n",
            "Epoch 143/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3222\n",
            "Epoch 144/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3459\n",
            "Epoch 145/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3088\n",
            "Epoch 146/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3418\n",
            "Epoch 147/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2826\n",
            "Epoch 148/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3198\n",
            "Epoch 149/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3020\n",
            "Epoch 150/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2904\n",
            "Epoch 151/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2937\n",
            "Epoch 152/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2780\n",
            "Epoch 153/200\n",
            "43/43 [==============================] - 1s 29ms/step - loss: 0.2940\n",
            "Epoch 154/200\n",
            "43/43 [==============================] - 1s 29ms/step - loss: 0.3019\n",
            "Epoch 155/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2704\n",
            "Epoch 156/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2804\n",
            "Epoch 157/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2862\n",
            "Epoch 158/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2742\n",
            "Epoch 159/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2859\n",
            "Epoch 160/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2828\n",
            "Epoch 161/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2479\n",
            "Epoch 162/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2396\n",
            "Epoch 163/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2591\n",
            "Epoch 164/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2722\n",
            "Epoch 165/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2908\n",
            "Epoch 166/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2629\n",
            "Epoch 167/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2474\n",
            "Epoch 168/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2508\n",
            "Epoch 169/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2214\n",
            "Epoch 170/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2426\n",
            "Epoch 171/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2426\n",
            "Epoch 172/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2166\n",
            "Epoch 173/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2521\n",
            "Epoch 174/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2318\n",
            "Epoch 175/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2365\n",
            "Epoch 176/200\n",
            "43/43 [==============================] - 1s 26ms/step - loss: 0.2404\n",
            "Epoch 177/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2309\n",
            "Epoch 178/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2513\n",
            "Epoch 179/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2195\n",
            "Epoch 180/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2182\n",
            "Epoch 181/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2209\n",
            "Epoch 182/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2254\n",
            "Epoch 183/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2294\n",
            "Epoch 184/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2267\n",
            "Epoch 185/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2364\n",
            "Epoch 186/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2450\n",
            "Epoch 187/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1953\n",
            "Epoch 188/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2111\n",
            "Epoch 189/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2293\n",
            "Epoch 190/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1985\n",
            "Epoch 191/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1983\n",
            "Epoch 192/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1834\n",
            "Epoch 193/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2023\n",
            "Epoch 194/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2175\n",
            "Epoch 195/200\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1911\n",
            "Epoch 196/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1865\n",
            "Epoch 197/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1870\n",
            "Epoch 198/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2153\n",
            "Epoch 199/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2187\n",
            "Epoch 200/200\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9740607650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEMyyYW1dEL4",
        "outputId": "2ea62ff2-d56a-438c-8a6c-ca83fa0ddf88"
      },
      "source": [
        "  # start = numpy.random.randint(0, len(network_input)-1)\n",
        "  start = 0 # выбираем индекс, начиная с которого для генерации будет использоваться существующая структура \n",
        "  pattern = copy.deepcopy(network_input[start]) # первые N нот не генерируются, а берутся из этого произведения\n",
        "\n",
        "  int_to_note = dict((number, note) for number, note in enumerate(notes_pitchnames))\n",
        "\n",
        "  predicted_outputs = []\n",
        "  predicted_speac = []\n",
        "\n",
        "  notes_count = 50\n",
        "  # generate n notes\n",
        "  for note_index in range(notes_count):\n",
        "        # print(\"------------\")\n",
        "        print(\"generating \", note_index, \" note\")\n",
        "        \n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 2))\n",
        "        prediction_input = copy.deepcopy(prediction_input)\n",
        "        # print(\"prediction_input1 = \", prediction_input)\n",
        "        # print(\"pattern1 = \", pattern)\n",
        "\n",
        "        for i in range(1, len(prediction_input[0]) + 1):\n",
        "            prediction_input[0][i - 1][0] /= float(n_vocab_notes) \n",
        "            prediction_input[0][i - 1][1] /= float(n_vocab_speac) \n",
        "\n",
        "        # print(\"prediction_input2 = \", prediction_input)\n",
        "        # print(\"pattern2 = \", pattern)\n",
        "\n",
        "        prediction_only_notes = model_only_notes.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index_only_notes = numpy.argmax(prediction_only_notes)\n",
        "        result_only_notes = int_to_note[index_only_notes]\n",
        "        predicted_outputs.append(result_only_notes)\n",
        "\n",
        "        next_speac = notes[start + sequence_length + note_index][1] \n",
        "        speac_to_number = sorted_speac_to_int[next_speac]\n",
        "        predicted_speac.append(next_speac)\n",
        "\n",
        "        pattern = numpy.append(pattern, [[index_only_notes, speac_to_number]], axis=0) \n",
        "        # pattern = numpy.append(pattern, [[index_only_notes / float(n_vocab_notes) , speac_to_number /float(n_vocab_speac)]], axis=0) # тут точно не надо делить???\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "        # print(\"pattern3 = \", pattern)\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating  0  note\n",
            "generating  1  note\n",
            "generating  2  note\n",
            "generating  3  note\n",
            "generating  4  note\n",
            "generating  5  note\n",
            "generating  6  note\n",
            "generating  7  note\n",
            "generating  8  note\n",
            "generating  9  note\n",
            "generating  10  note\n",
            "generating  11  note\n",
            "generating  12  note\n",
            "generating  13  note\n",
            "generating  14  note\n",
            "generating  15  note\n",
            "generating  16  note\n",
            "generating  17  note\n",
            "generating  18  note\n",
            "generating  19  note\n",
            "generating  20  note\n",
            "generating  21  note\n",
            "generating  22  note\n",
            "generating  23  note\n",
            "generating  24  note\n",
            "generating  25  note\n",
            "generating  26  note\n",
            "generating  27  note\n",
            "generating  28  note\n",
            "generating  29  note\n",
            "generating  30  note\n",
            "generating  31  note\n",
            "generating  32  note\n",
            "generating  33  note\n",
            "generating  34  note\n",
            "generating  35  note\n",
            "generating  36  note\n",
            "generating  37  note\n",
            "generating  38  note\n",
            "generating  39  note\n",
            "generating  40  note\n",
            "generating  41  note\n",
            "generating  42  note\n",
            "generating  43  note\n",
            "generating  44  note\n",
            "generating  45  note\n",
            "generating  46  note\n",
            "generating  47  note\n",
            "generating  48  note\n",
            "generating  49  note\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUWvsCz8T_77"
      },
      "source": [
        "import music21\n",
        "from music21 import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVQEkNOORL3f"
      },
      "source": [
        "create_new_midi(predicted_outputs, \"speac_example_n=\" + str(sequence_length) + \".mid\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}